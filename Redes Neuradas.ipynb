{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passo a passo de redes neurais \n",
    "\n",
    "Neste notebook, iremos implementar uma rede neural de uma única camada escondida 'na mão'. \n",
    "\n",
    "Este notebook foi baseado no curso do Andre NG (Univ. Stanford) sobre Deep Learning, disponível no Coursera e no notebook https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Planar%20data%20classification%20with%20one%20hidden%20layer.ipynb\n",
    "\n",
    "**Funções auxiliares**: \n",
    "planar_utils.py contém funções úteis para import e visualização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "#from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets # funções auxiliares\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y[0], cmap=plt.cm.Spectral)\n",
    "    \n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def load_planar_dataset():\n",
    "    np.random.seed(1)\n",
    "    m = 400 # number of examples\n",
    "    N = int(m/2) # number of points per class\n",
    "    D = 2 # dimensionality\n",
    "    X = np.zeros((m,D)) # data matrix where each row is a single example\n",
    "    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
    "    a = 4 # maximum ray of the flower\n",
    "\n",
    "    for j in range(2):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
    "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        Y[ix] = j\n",
    "        \n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def load_extra_datasets():  \n",
    "    N = 200\n",
    "    noisy_circles = sklearn.datasets.make_circles(n_samples=N, factor=.5, noise=.3)\n",
    "    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)\n",
    "    blobs = sklearn.datasets.make_blobs(n_samples=N, random_state=5, n_features=2, centers=6)\n",
    "    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)\n",
    "    no_structure = np.random.rand(N, 2), np.random.rand(N, 2)\n",
    "    \n",
    "    return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_planar_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando o dataset, que contém duas features e um output categórico. O objetivo deste exercício é criar um modelo para classificação destas categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[0, :], X[1, :], c=Y[0], s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conferindo o shape\n",
    "shape_X = X.shape\n",
    "shape_Y = Y.shape\n",
    "print(shape_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Regressão Logística\n",
    "\n",
    "Para compararmos os resultados, utilizaremos uma regressão logistica inicialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando regressão logística\n",
    "clf = sklearn.linear_model.LogisticRegressionCV()\n",
    "clf.fit(X.T, Y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para plotar a fronteira de decisão do problema, utilizaremos uma função que importamos das funções auxiliares, a 'plot_decision_boundary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: clf.predict(x), X, Y)\n",
    "plt.title(\"Regressão Logística\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LR_predictions = clf.predict(X.T)\n",
    "print ('Acurácia da Regressão no training set: %d ' % float((np.dot(Y, LR_predictions) + np.dot(1 - Y,1 - LR_predictions)) / float(Y.size) * 100) +\n",
    "'% - lembre...nunca avalie um modelo no training set!!! :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Modelo de Redes Neurais\n",
    "\n",
    "Vamos tentar uma rede neural, para capturar as não linearidades do problema.\n",
    "\n",
    "<img src=\"classification_kiank.png\">\n",
    "\n",
    "\n",
    "**Matematicamente**:\n",
    "\n",
    "Para um exemplo $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "Com as predições de todos os exemplos, podemos também calcular $J$, a função custo final:\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
    "\n",
    "\n",
    "A metodologia para construir uma rede neural é:\n",
    "1. Definir a arquitetura (inputs, hidden layers)\n",
    "2. Inicializar os parâmetros do modelo\n",
    "3. Loop\n",
    "    - Forward Propagation\n",
    "    - Computar Loss e Função custo\n",
    "    - BackPropagation\n",
    "    - Gradiente descendente (update nos parâmetros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Arquitetura da Rede ####\n",
    "\n",
    "\n",
    "- n_x: tamanho da camada de input (2)\n",
    "- n_h: tamanho do hidden layer (default 4) \n",
    "- n_y: tamanho da camada de saída (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y, hidden = 4):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"    \n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = hidden\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "    \n",
    "    return (n_x, n_h, n_y)\n",
    "\n",
    "n_x, n_h, n_y = layer_sizes(X,Y)\n",
    "print(\"Tamanho do Input: \" + str(n_x))\n",
    "print(\"Tamanho do Hidden: \" + str(n_h))\n",
    "print(\"Tamanho do Output: \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Inicializando os parâmetros do modelo ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    n_x -- tamanho do input layer\n",
    "    n_h -- tamanho do hidden layer\n",
    "    n_y -- tamanho do output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros(shape=(n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros(shape=(n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Forward Propagation ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    X -- input de dados, tamanho (n_x, m). No caso, 2 por 400\n",
    "    parameters -- dicionário contendo todos os parâmetros utilizados no problema\n",
    "    \n",
    "    Resultados:\n",
    "    A2 -- output da sigmoid da segunda função de ativação - ou seja, a predição\n",
    "    cache -- dicionário contendo Z1, A1, Z2 and A2. Será utilizando no back propagation\n",
    "    \"\"\"\n",
    "    # Pegando cada parâmetro do dicionário\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Forward Propagation para calcular A2 (estimativas)\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2, cache = forward_propagation(X, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos o $A^{[2]}$, ou seja, a estimativa final, podemos calcular a função custo, da seguinte forma (log loss)\n",
    "\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Função Custo ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"   \n",
    "    \n",
    "    Argumentos:\n",
    "    A2 -- output da sigmoid da segunda função de ativação - ou seja, a predição (vetor de tamanho n)\n",
    "    Y -- valor verdadeiro do label (vetor de tamanho n)\n",
    "    parameters -- dicionário contendo W1, b1, W2 and b2\n",
    "    \n",
    "    Resultados:\n",
    "    cost -- log loss de acordo com equação (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "        \n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect.                                     \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A2, Y_assess, parameters = compute_cost_test_case()\n",
    "\n",
    "print(\"custo = \" + str(compute_cost(A2, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Backward Propagation ####\n",
    "\n",
    "<img src=\"grad_summary.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "<!--\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (a^{[2](i)} - y^{(i)})$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T} $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$\n",
    "\n",
    "- Note that $*$ denotes elementwise multiplication.\n",
    "- The notation you will use is common in deep learning coding:\n",
    "    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
    "    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
    "    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
    "    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n",
    "    \n",
    "!-->\n",
    "\n",
    "- Tips:\n",
    "    - To compute dZ1 you'll need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So you can compute \n",
    "    $g^{[1]'}(Z^{[1]})$ using `(1 - np.power(A1, 2))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"    \n",
    "    Argumentos:\n",
    "    parameters -- dicionário com os parâmetros\n",
    "    cache -- dicionário contendo \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input de dados, tamanho (n_x, m). No caso, 2 por 400\n",
    "    Y -- valor verdadeiro do label (vetor de tamanho n)\n",
    "    \n",
    "    Resultado:\n",
    "    grads -- dicionário contendo os gradientes dos parâmetros (Será usado no gradiente descendente)\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Pegando W1 e W2 do dicionário de parâmetro e A1 e A2 do dicionário de cache\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "\n",
    "    \n",
    "    # Cálculo do backpropagation \n",
    "    dZ2= A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)    \n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = backward_propagation(parameters, cache, X, Y)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
    "print (\"db2 = \"+ str(grads[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que esses valores representam? É como a função custo varia com a variação de cada parâmetro. Valores negativos indicam que, ao aumentar o valor do parâmetro w ou b, diminuimos a função custo final. De maneira análoga, um valor positivo signifca que diminuindo o valor do parâmetro, diminuimos a função custo! Essa é a matéria prima para o gradiente descendente: como a função custo varia com cada parâmetro.\n",
    "\n",
    "Como exemplo, considere o primeiro valor de dW1, que representa o gradiente do peso do primeiro neurônio de input com o primeiro neurônio da camada escondida. O valor negativo indica que, ao aumentar o peso, diminuimos a função custo. Vamos ver se isso é verdade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Peso antes: \" + str(parameters['W1'][1][1]))\n",
    "A2, cache = forward_propagation(X, parameters)\n",
    "print(\"Custo antes = \" + str(compute_cost(A2, Y)))\n",
    "\n",
    "# Adicionando um pequeno valor positivo ao primeiro peso\n",
    "parameters['W1'][1][1] = parameters['W1'][1][1] + 0.01\n",
    "\n",
    "A2, cache = forward_propagation(X, parameters)\n",
    "print(\"Peso depois: \" + str(parameters['W1'][1][1]))\n",
    "print(\"Custo depois = \" + str(compute_cost(A2, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 - Gradiente Descendente ####\n",
    "\n",
    "**Gradiente Descendente**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ em que $\\alpha$ é o learning rate e $\\theta$ são os parâmetros.\n",
    "\n",
    "**Ilustração**: Gradiente descendente com learning rate bom e ruim. Cortesia de Adam Harley.\n",
    "\n",
    "<img src=\"sgd.gif\" style=\"width:400;height:400;\"> <img src=\"sgd_bad.gif\" style=\"width:400;height:400;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate=1.2):\n",
    "    \"\"\"\n",
    "    Atualiza os parâmetros utilizando gradiente descendente\n",
    "    \n",
    "    Argumentos:\n",
    "    parameters -- dicionário com os parâmetros \n",
    "    grads -- dicionário contendo os gradientes dos parâmetros\n",
    "    \n",
    "    Resultados:\n",
    "    parameters -- dicionário python com os parâmetros atualizados\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    # Update rule for each parameter    \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Criando sua própria rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, epochs=1000, batch_size = 100, print_cost=False):\n",
    "    import pandas as pd\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dos dados\n",
    "    Y -- labels\n",
    "    n_h -- tamanho da hidden layer\n",
    "    epochs -- número de épocas\n",
    "    batch_size -- tamanho do batch\n",
    "    print_cost -- se True, mostra o custo a cada 1000 iterações\n",
    "    \n",
    "    Resultados:\n",
    "    parameters -- parâmetros aprendidos pelo modelo. Podem ser utilizados para fazer 'predict'\n",
    "    \"\"\"\n",
    "    batches_init = pd.Series(range(0, X.shape[1], batch_size))\n",
    "    batches_end = batches_init-1\n",
    "    batches_end = batches_end[1:].append(pd.Series(X.shape[1])-1, ignore_index = True)\n",
    "\n",
    "\n",
    "    np.random.seed(1)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "\n",
    "    # Inicializando parâmetros\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']  \n",
    "\n",
    "    # loop nas épocas\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Loop nos batches\n",
    "        for id_batch in range(0,len(batches_init)):        \n",
    "            X_batch = X[:,batches_init[id_batch]:batches_end[id_batch]]    \n",
    "            Y_batch = Y[:,batches_init[id_batch]:batches_end[id_batch]]    \n",
    "\n",
    "            # Forward propagation.\n",
    "            A2, cache = forward_propagation(X_batch, parameters)\n",
    "\n",
    "            # Cost function.\n",
    "            cost = compute_cost(A2, Y_batch)        \n",
    "\n",
    "            # Backpropagation.\n",
    "            grads = backward_propagation(parameters, cache, X_batch, Y_batch)    \n",
    "\n",
    "            # Gradient descent. Atualiza os parâmetros\n",
    "            parameters = update_parameters(parameters, grads, learning_rate=0.1)\n",
    "\n",
    "            # Print do erro\n",
    "            if print_cost and i % 1000 == 0:\n",
    "                print('.' ,end =\" \")\n",
    "                if id_batch == len(batches_init)-1:\n",
    "                    print (\"Custo depois da época %i: %f\" % (i, cost))\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = nn_model(X, Y, 4, epochs=10000, batch_size = 100, print_cost=True)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Predição\n",
    "\n",
    "\n",
    "**Função 'predict'**: $y_{prediction} = \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Função de 'predict'. Usa apenas o forward propagation!\n",
    "    \n",
    "    Arguments:\n",
    "    parametros -- dicionario contendo os parâmetros do modelo\n",
    "    X -- data set de input\n",
    "    \n",
    "    Returnos\n",
    "    predições -- vetor de predições do modelo (0/1)\n",
    "    \"\"\"\n",
    "\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.round(A2)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plota a fronteira de decisão\n",
    "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(parameters, X)\n",
    "print ('Acurácia: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
